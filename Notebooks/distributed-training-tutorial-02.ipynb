{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install datasets\n# !pip install transformers","metadata":{"id":"XmbX6vPI-89p","outputId":"849ed5fa-0bd1-4e01-dcbc-ec406a6c40b0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nimport numpy as np\nfrom tqdm import tqdm\nimport tensorflow as tf\nfrom datasets import load_dataset, concatenate_datasets\nfrom transformers import AutoTokenizer, TFAutoModelForSequenceClassification, DataCollatorWithPadding","metadata":{"id":"rdfCWnP0-rWW","execution":{"iopub.status.busy":"2022-06-06T10:40:38.531802Z","iopub.execute_input":"2022-06-06T10:40:38.532184Z","iopub.status.idle":"2022-06-06T10:40:48.289357Z","shell.execute_reply.started":"2022-06-06T10:40:38.532084Z","shell.execute_reply":"2022-06-06T10:40:48.288248Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2022-06-06 10:40:39.322244: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n2022-06-06 10:40:39.322416: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Detect hardware\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\nexcept ValueError:\n    tpu = None\n    gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n    \n# Select appropriate distribution strategy\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu) # Going back and forth between TPU and host is expensive. Better to run 128 batches on the TPU before reporting back.\n    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])  \nelif len(gpus) > 1:\n    strategy = tf.distribute.MirroredStrategy([gpu.name for gpu in gpus])\n    print('Running on multiple GPUs ', [gpu.name for gpu in gpus])\nelif len(gpus) == 1:\n    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n    print('Running on single GPU ', gpus[0].name)\nelse:\n    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n    print('Running on CPU')\nprint(\"Number of accelerators: \", strategy.num_replicas_in_sync)","metadata":{"id":"wpG3pQ-JISZU","outputId":"1e37d4d9-bb60-4612-fb43-5637b9363c2f","execution":{"iopub.status.busy":"2022-06-06T10:40:48.291180Z","iopub.execute_input":"2022-06-06T10:40:48.291483Z","iopub.status.idle":"2022-06-06T10:40:54.862181Z","shell.execute_reply.started":"2022-06-06T10:40:48.291443Z","shell.execute_reply":"2022-06-06T10:40:54.861229Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2022-06-06 10:40:48.302973: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2022-06-06 10:40:48.306054: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n2022-06-06 10:40:48.306092: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n2022-06-06 10:40:48.306119: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (70435bc77a3c): /proc/driver/nvidia/version does not exist\n2022-06-06 10:40:48.309757: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-06-06 10:40:48.311291: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2022-06-06 10:40:48.349782: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}\n2022-06-06 10:40:48.349859: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:30042}\n2022-06-06 10:40:48.369785: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}\n2022-06-06 10:40:48.369884: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:30042}\n2022-06-06 10:40:48.371602: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:30042\n","output_type":"stream"},{"name":"stdout","text":"Running on TPU  ['10.0.0.2:8470']\nNumber of accelerators:  8\n","output_type":"stream"}]},{"cell_type":"code","source":"# define batch size\nbatch_size_per_replica = 16\nbatch_size = batch_size_per_replica * strategy.num_replicas_in_sync\nprint('Batch size:', batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T10:40:54.863376Z","iopub.execute_input":"2022-06-06T10:40:54.863638Z","iopub.status.idle":"2022-06-06T10:40:54.869582Z","shell.execute_reply.started":"2022-06-06T10:40:54.863611Z","shell.execute_reply":"2022-06-06T10:40:54.868682Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Batch size: 128\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset = load_dataset(\"imdb\")\ndataset['valid'] = dataset.pop('test')\ndataset.pop('unsupervised')\ndataset","metadata":{"id":"pnDHFvjZ-yp_","outputId":"f20381d4-0a07-4e3a-f60a-381e82372da4","execution":{"iopub.status.busy":"2022-06-06T10:40:54.871650Z","iopub.execute_input":"2022-06-06T10:40:54.872488Z","iopub.status.idle":"2022-06-06T10:41:41.286501Z","shell.execute_reply.started":"2022-06-06T10:40:54.872455Z","shell.execute_reply":"2022-06-06T10:41:41.285826Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.92k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf3c25bec6944128bd5e50a45fd8e93c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.05k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"066594e264ca44f39d2917c9ad5e627f"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.02 MiB, post-processed: Unknown size, total: 207.25 MiB) to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/84.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddaed13e0d4e4998bb0f8b1d585eec11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset imdb downloaded and prepared to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebf6b7ba70cf4ddb9270e1c968344f67"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    valid: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"sample_train = dataset['train'].shuffle(seed=32).select(range(1601))\nsample_valid = dataset['valid'].shuffle(seed=32).select(range(1600))\n\nfinal_ds = sample_train.train_test_split(train_size=1600)\nfinal_ds['valid'] = sample_valid\nfinal_ds.pop('test')\nfinal_ds","metadata":{"id":"NAMwHYdjK_CR","outputId":"052d10ea-6575-4935-eb24-1903c297863d","execution":{"iopub.status.busy":"2022-06-06T10:41:41.287940Z","iopub.execute_input":"2022-06-06T10:41:41.288680Z","iopub.status.idle":"2022-06-06T10:41:41.456857Z","shell.execute_reply.started":"2022-06-06T10:41:41.288645Z","shell.execute_reply":"2022-06-06T10:41:41.456186Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 1600\n    })\n    valid: Dataset({\n        features: ['text', 'label'],\n        num_rows: 1600\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"checkpoint = 'distilbert-base-uncased'\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)","metadata":{"id":"5Q17nqorBH-M","execution":{"iopub.status.busy":"2022-06-06T10:41:41.458304Z","iopub.execute_input":"2022-06-06T10:41:41.458753Z","iopub.status.idle":"2022-06-06T10:41:43.713432Z","shell.execute_reply.started":"2022-06-06T10:41:41.458723Z","shell.execute_reply":"2022-06-06T10:41:43.712316Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04929a6faab84e758087c54e1908273c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e70a860b7cb14bf59ba947aed69b6bc3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4892fb3f933e4f6ab94f089b3e21593d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b7a8f7c564e42ba879cc2e0a44f1c0b"}},"metadata":{}}]},{"cell_type":"code","source":"model_max_len = tokenizer.model_max_length\nmodel_max_len","metadata":{"id":"w7HY2uiCFQ_5","outputId":"a6875c6a-8eb0-4e8d-c534-c9de31d7e969","execution":{"iopub.status.busy":"2022-06-06T10:41:43.715563Z","iopub.execute_input":"2022-06-06T10:41:43.717257Z","iopub.status.idle":"2022-06-06T10:41:43.725847Z","shell.execute_reply.started":"2022-06-06T10:41:43.717224Z","shell.execute_reply":"2022-06-06T10:41:43.724569Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"512"},"metadata":{}}]},{"cell_type":"code","source":"def tokenize_function(examples):\n  examples = [example.lower() for example in examples['text']]\n  return tokenizer(examples, max_length=model_max_len, padding=True, truncation=True)\n\ntokenized_dataset = final_ds.map(tokenize_function, batched=True, remove_columns=['text'])\ntokenized_dataset","metadata":{"id":"clC9mt5-ETeM","outputId":"4c642a86-b1c2-4de1-f701-8f26a45d1c1c","execution":{"iopub.status.busy":"2022-06-06T10:41:43.727513Z","iopub.execute_input":"2022-06-06T10:41:43.728417Z","iopub.status.idle":"2022-06-06T10:41:46.373455Z","shell.execute_reply.started":"2022-06-06T10:41:43.728352Z","shell.execute_reply":"2022-06-06T10:41:46.372397Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92f20e4715054098821a8983f941845d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a018e122b28049d5886c74a99cd9d7b9"}},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['attention_mask', 'input_ids', 'label'],\n        num_rows: 1600\n    })\n    valid: Dataset({\n        features: ['attention_mask', 'input_ids', 'label'],\n        num_rows: 1600\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors='tf')\n\ntf_train_ds = tokenized_dataset['train'].to_tf_dataset(\n    columns=['input_ids', 'attention_mask'],\n    label_cols=['label'],\n    shuffle=True,\n    collate_fn=data_collator,\n    batch_size=batch_size,)\n\ntf_valid_ds = tokenized_dataset['valid'].to_tf_dataset(\n    columns=['input_ids', 'attention_mask'],\n    label_cols=['label'],\n    shuffle=False,\n    collate_fn=data_collator,\n    batch_size=batch_size,)","metadata":{"id":"SfuiY2fBFEe5","outputId":"9670e89d-144d-4398-f71b-59b6d3ff42db","execution":{"iopub.status.busy":"2022-06-06T10:29:03.549250Z","iopub.execute_input":"2022-06-06T10:29:03.549845Z","iopub.status.idle":"2022-06-06T10:29:05.276400Z","shell.execute_reply.started":"2022-06-06T10:29:03.549807Z","shell.execute_reply":"2022-06-06T10:29:05.275505Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"for x in tf_train_ds.take(1):\n  print(x)","metadata":{"id":"GvJqeTo7FEhS","outputId":"ac6c8742-9c48-46b1-8f9b-7dcac02e3652","execution":{"iopub.status.busy":"2022-06-06T10:29:05.277957Z","iopub.execute_input":"2022-06-06T10:29:05.278639Z","iopub.status.idle":"2022-06-06T10:29:05.368414Z","shell.execute_reply.started":"2022-06-06T10:29:05.278597Z","shell.execute_reply":"2022-06-06T10:29:05.367596Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"({'input_ids': <tf.Tensor: shape=(16, 512), dtype=int64, numpy=\narray([[ 101, 1045, 2034, ..., 2007, 7928,  102],\n       [ 101, 7543, 2028, ...,    0,    0,    0],\n       [ 101, 2106, 3087, ...,    0,    0,    0],\n       ...,\n       [ 101, 2000, 2404, ...,    0,    0,    0],\n       [ 101, 1012, 1012, ...,    0,    0,    0],\n       [ 101, 2023, 3185, ...,    0,    0,    0]])>, 'attention_mask': <tf.Tensor: shape=(16, 512), dtype=int64, numpy=\narray([[1, 1, 1, ..., 1, 1, 1],\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       ...,\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0]])>}, <tf.Tensor: shape=(16,), dtype=int64, numpy=array([1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1])>)\n","output_type":"stream"},{"name":"stderr","text":"2022-06-06 10:29:05.318998: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"warnings.filterwarnings('ignore')\nwith strategy.scope():\n  model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=1)\n  model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n                optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5))\n\nnum_epochs = 2\nfor epoch in range(num_epochs):\n    print(f'Epoch {epoch+1}/{num_epochs} ====================>')\n    train_losses = []\n    for x, y in tqdm(tf_train_ds, total=len(tf_train_ds)):\n        train_results = model.train_on_batch(x, y, \n                                             reset_metrics=True, \n                                             return_dict=True)\n        train_losses.append(train_results['loss'])\n    print('Train Loss:', np.mean(train_losses))\n        \n    valid_losses = []\n    for x, y in tqdm(tf_valid_ds, total=len(tf_valid_ds)):\n        valid_results = model.test_on_batch(x, y, \n                                             reset_metrics=True, \n                                             return_dict=True)\n        valid_losses.append(valid_results['loss'])\n    print('Valid Loss:', np.mean(valid_losses))","metadata":{"id":"dkB-1ss-EThD","outputId":"0231478f-424a-409d-ba32-bc2410e4469a","execution":{"iopub.status.busy":"2022-06-06T10:37:33.171107Z","iopub.execute_input":"2022-06-06T10:37:33.171629Z","iopub.status.idle":"2022-06-06T10:40:00.186603Z","shell.execute_reply.started":"2022-06-06T10:37:33.171584Z","shell.execute_reply":"2022-06-06T10:40:00.185749Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_layer_norm', 'vocab_transform', 'vocab_projector']\n- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_99', 'classifier', 'pre_classifier']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/2 ====================>\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:57<00:00,  1.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.46231755912303923\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:18<00:00,  5.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Valid Loss: 0.32741664819419386\nEpoch 2/2 ====================>\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:53<00:00,  1.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.22769431360065936\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:17<00:00,  5.82it/s]","output_type":"stream"},{"name":"stdout","text":"Valid Loss: 0.3345218504592776\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# TPU","metadata":{}},{"cell_type":"code","source":"train_input_ids = tf.data.Dataset.from_tensor_slices(tf.constant(tokenized_dataset['train']['input_ids']))\ntrain_attention_mask = tf.data.Dataset.from_tensor_slices(tf.constant(tokenized_dataset['train']['attention_mask']))\ntrain_labels = tf.data.Dataset.from_tensor_slices(tf.constant(tokenized_dataset['train']['label']))\ntrain_inputs = {'input_ids': train_input_ids, 'attention_mask':train_attention_mask}\ntrain_dataset = tf.data.Dataset.zip((train_inputs, train_labels)).shuffle(512).batch(batch_size).prefetch(-1)\n\n\nvalid_input_ids = tf.data.Dataset.from_tensor_slices(tf.constant(tokenized_dataset['valid']['input_ids']))\nvalid_attention_mask = tf.data.Dataset.from_tensor_slices(tf.constant(tokenized_dataset['valid']['attention_mask']))\nvalid_labels = tf.data.Dataset.from_tensor_slices(tf.constant(tokenized_dataset['valid']['label']))\nvalid_inputs = {'input_ids': valid_input_ids, 'attention_mask':valid_attention_mask}\nvalid_dataset = tf.data.Dataset.zip((valid_inputs, valid_labels)).batch(batch_size).prefetch(-1)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T10:41:51.031572Z","iopub.execute_input":"2022-06-06T10:41:51.032345Z","iopub.status.idle":"2022-06-06T10:41:53.225305Z","shell.execute_reply.started":"2022-06-06T10:41:51.032297Z","shell.execute_reply":"2022-06-06T10:41:53.224391Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n  model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=1)\n  model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n                optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5))\n  \nnum_epochs = 2\nfor epoch in range(num_epochs):\n    print(f'Epoch {epoch+1}/{num_epochs} ====================>')\n    train_losses = []\n    for x, y in tqdm(train_dataset, total=len(train_dataset)):\n        train_results = model.train_on_batch(x, y, \n                                             reset_metrics=True, \n                                             return_dict=True)\n        train_losses.append(train_results['loss'])\n    print('Train Loss:', np.mean(train_losses))\n        \n    valid_losses = []\n    for x, y in tqdm(valid_dataset, total=len(valid_dataset)):\n        valid_results = model.test_on_batch(x, y, \n                                             reset_metrics=True, \n                                             return_dict=True)\n        valid_losses.append(valid_results['loss'])\n    print('Valid Loss:', np.mean(valid_losses))","metadata":{"id":"dRKxPhBZIvx3","execution":{"iopub.status.busy":"2022-06-06T10:42:41.272297Z","iopub.execute_input":"2022-06-06T10:42:41.272613Z","iopub.status.idle":"2022-06-06T10:44:31.218989Z","shell.execute_reply.started":"2022-06-06T10:42:41.272584Z","shell.execute_reply":"2022-06-06T10:44:31.217036Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_layer_norm', 'vocab_transform', 'activation_13', 'vocab_projector']\n- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_39', 'classifier', 'pre_classifier']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/2 ====================>\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13/13 [01:16<00:00,  5.92s/it]\n2022-06-06 10:44:04.913935: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:57] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find the relevant tensor remote_handle: Op ID: 22656, Output num: 0\nAdditional GRPC error information from remote target /job:worker/replica:0/task:0:\n:{\"created\":\"@1654512244.909995306\",\"description\":\"Error received from peer ipv4:10.0.0.2:8470\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Unable to find the relevant tensor remote_handle: Op ID: 22656, Output num: 0\",\"grpc_status\":3}\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6468214484361502\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13/13 [00:14<00:00,  1.09s/it]\n","output_type":"stream"},{"name":"stdout","text":"Valid Loss: 0.4889642275296725\nEpoch 2/2 ====================>\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13/13 [00:06<00:00,  2.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.37460579780431896\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13/13 [00:05<00:00,  2.25it/s]","output_type":"stream"},{"name":"stdout","text":"Valid Loss: 0.2735814612645369\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}